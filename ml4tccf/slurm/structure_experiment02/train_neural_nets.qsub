#!/bin/bash

#SBATCH --job-name="train_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="gpuwf"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=48:00:00
#SBATCH --array=0-39
#SBATCH --exclude=h28n12
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=train_neural_nets_%A_%a.out

# Load the CUDA library, which uses GPUs (graphics-processing units) to accelerate training of the neural network.  You do not need to install this library yourself; it is already installed on Hera.  You just need to load the module at the beginning of any Slurm script used to train a neural network.
module load cuda/12.3.1

# Load your Python environment (the one you installed with my instructions).
conda init
conda activate base

# Make sure the Slurm script is using your Python environment, not someone else's.  Each "echo" command will print something to the log file.  There's a good chance that the first "echo" command will print an incomprehensible blob of text, but the second echo command should print a path to a Python executable in one of your directories (the path should be /scratch1/RDARCH/rda-ghpcs/Yuehli.Chen...).
echo `which conda`
echo `which python3`

# Directory with the Python script that I want to run (train_neural_net_structure.py).
CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_standalone/ml4tccf"

# Directory containing neural-network templates (the pre-defined architecture for every neural network I want to train).
TEMPLATE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_models/structure_experiment02/templates"

# Output directory (trained neural networks will be saved here).
TOP_OUTPUT_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_models/structure_experiment02"

# Directories with training and validation data for neural networks.
TRAINING_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_project/satellite_data/processed/normalized_for_paper/shuffled_8hour_chunks/training/800x800_grids"
VALIDATION_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_project/satellite_data/processed/normalized_for_paper/shuffled_8hour_chunks/validation/800x800_grids"
A_DECK_FILE_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_project/a_decks/including_western_pacific_a-decks_not-b-decks/processed_a_decks_2016-2022_normalized_structure.nc"

# Each of the five lines below defines an array with 40 elements.  This is because I want to train 40 neural networks.  For the [k]th neural network, hyperparameters are given by the [k]th element of GRAD_ACCUM_STEP_COUNTS, the [k]th element of TRAINING_BATCH_PER_EPOCH_COUNTS, the [k]th element of VALIDATION_BATCH_PER_EPOCH_COUNTS, the [k]th element of USE_PHYS_CONSTRAINT_FLAGS, and the [k]th element of DENSE_DROPOUT_RATES.  For example, the first neural network will have 1 gradient-accumulation step, 48 training batches per epoch, 24 validation batches per epoch, no physical constraints (because the Boolean flag is 0 instead of 1), and a dropout rate of 0.125 for dense layers.
GRAD_ACCUM_STEP_COUNTS=("1" "2" "3" "4" "5" "1" "2" "3" "4" "5" "1" "2" "3" "4" "5" "1" "2" "3" "4" "5" "1" "2" "3" "4" "5" "1" "2" "3" "4" "5" "1" "2" "3" "4" "5" "1" "2" "3" "4" "5")
TRAINING_BATCH_PER_EPOCH_COUNTS=("48" "24" "16" "12" "10" "48" "24" "16" "12" "10" "48" "24" "16" "12" "10" "48" "24" "16" "12" "10" "48" "24" "16" "12" "10" "48" "24" "16" "12" "10" "48" "24" "16" "12" "10" "48" "24" "16" "12" "10")
VALIDATION_BATCH_PER_EPOCH_COUNTS=("24" "12" "8" "6" "5" "24" "12" "8" "6" "5" "24" "12" "8" "6" "5" "24" "12" "8" "6" "5" "24" "12" "8" "6" "5" "24" "12" "8" "6" "5" "24" "12" "8" "6" "5" "24" "12" "8" "6" "5")
USE_PHYS_CONSTRAINT_FLAGS=("0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "0" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1")
DENSE_DROPOUT_RATES=("0.125" "0.125" "0.125" "0.125" "0.125" "0.250" "0.250" "0.250" "0.250" "0.250" "0.375" "0.375" "0.375" "0.375" "0.375" "0.500" "0.500" "0.500" "0.500" "0.500" "0.125" "0.125" "0.125" "0.125" "0.125" "0.250" "0.250" "0.250" "0.250" "0.250" "0.375" "0.375" "0.375" "0.375" "0.375" "0.500" "0.500" "0.500" "0.500" "0.500")

# SLURM_ARRAY_TASK_ID is the current array index.  You'll notice that I didn't define this variable anywhere in the script; Slurm defines it for me.  The line "#SBATCH --array=0-39" in the header tells Slurm that this is an array job, with the array index running from 0 to 39.  When you submit this script -- using "sbatch train_neural_nets.qsub" -- it will launch 40 jobs simultaneously.  Each job will have a major ID and a sub-ID.  The major ID is assigned randomly, but the sub-IDs will run from 0 to 39.  For example, if the major ID ends up being 1234567, the full job IDs will be 1234567_0, 1234567_1, 1234567_2, ..., 1234567_39.

# Each of the five lines below sets hyperparameters, according to the current array index.
num_grad_accum_steps=${GRAD_ACCUM_STEP_COUNTS[$SLURM_ARRAY_TASK_ID]}
num_training_batches_per_epoch=${TRAINING_BATCH_PER_EPOCH_COUNTS[$SLURM_ARRAY_TASK_ID]}
num_validation_batches_per_epoch=${VALIDATION_BATCH_PER_EPOCH_COUNTS[$SLURM_ARRAY_TASK_ID]}
use_phys_constraints=${USE_PHYS_CONSTRAINT_FLAGS[$SLURM_ARRAY_TASK_ID]}
dense_dropout_rate=${DENSE_DROPOUT_RATES[$SLURM_ARRAY_TASK_ID]}

# The next two lines use hyperparameters (based on the current array index) to find the input file and output directory.
template_file_name="${TEMPLATE_DIR_NAME}/num-grad-accum-steps=${num_grad_accum_steps}_use-phys-constraints=${use_phys_constraints}_dense-dropout-rate=${dense_dropout_rate}/model.h5"
output_dir_name="${TOP_OUTPUT_DIR_NAME}/num-grad-accum-steps=${num_grad_accum_steps}_use-phys-constraints=${use_phys_constraints}_dense-dropout-rate=${dense_dropout_rate}"

# This command calls the Python script with the relevant input arguments.
python3 -u "${CODE_DIR_NAME}/train_neural_net_structure.py" \
--input_template_file_name="${template_file_name}" \
--output_model_dir_name="${output_dir_name}" \
--lag_times_minutes 0 30 60 90 120 150 180 \
--low_res_wavelengths_microns 8.500 9.610 12.300 \
--num_examples_per_batch=15 \
--num_rows_low_res=800 \
--num_columns_low_res=800 \
--synoptic_times_only=0 \
--a_deck_file_name="${A_DECK_FILE_NAME}" \
--scalar_a_deck_field_names "none" \
--remove_nontropical_systems=0 \
--remove_tropical_systems=0 \
--target_field_names "intensity_kt" "radius_of_34kt_wind_km" "radius_of_50kt_wind_km" "radius_of_64kt_wind_km" "radius_of_max_wind_km" \
--target_file_name="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_project/extended_best_track_thru_nov2023.nc" \
--target_shrink_factor=0.01 \
--do_residual_prediction=0 \
--satellite_dir_name_for_training="${TRAINING_DIR_NAME}" \
--training_years 2016 2017 2018 2019 2020 2021 2022 \
--satellite_dir_name_for_validation="${VALIDATION_DIR_NAME}" \
--validation_years 2016 2017 2018 2019 2020 2021 2022 \
--num_epochs=1000 \
--num_training_batches_per_epoch=${num_training_batches_per_epoch} \
--num_validation_batches_per_epoch=${num_validation_batches_per_epoch} \
--plateau_patience_epochs=10 \
--plateau_learning_rate_multiplier=0.9 \
--early_stopping_patience_epochs=500
