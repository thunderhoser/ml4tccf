#!/bin/tcsh

#SBATCH --job-name="apply_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
##SBATCH --nodes=1
##SBATCH --ntasks=1
##SBATCH --cpus-per-task=1
##SBATCH --ntasks-per-node=1
#SBATCH --time=30:00:00
#SBATCH --array=1-100%70
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=apply_neural_nets_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_standalone/ml4tccf"
set TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_models/experiment02_batches_and_channels"
set TRAINING_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_project/satellite_data/processed/normalized_params_from_2017-2019"

set FIRST_LAYER_CHANNEL_COUNTS=("04" "04" "04" "04" "08" "08" "08" "08" "12" "12" "12" "12" "16" "16" "16" "16" "20" "20" "20" "20" "04" "04" "04" "04" "08" "08" "08" "08" "12" "12" "12" "12" "16" "16" "16" "16" "20" "20" "20" "20" "04" "04" "04" "04" "08" "08" "08" "08" "12" "12" "12" "12" "16" "16" "16" "16" "20" "20" "20" "20" "04" "04" "04" "04" "08" "08" "08" "08" "12" "12" "12" "12" "16" "16" "16" "16" "20" "20" "20" "20" "04" "04" "04" "04" "08" "08" "08" "08" "12" "12" "12" "12" "16" "16" "16" "16" "20" "20" "20" "20")
set BATCHES_PER_UPDATE_COUNTS=("02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "02" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "04" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "08" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10" "10")
set MAX_EXAMPLES_PER_CYCLONE_COUNTS=("1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4")

set num_first_layer_channels=${FIRST_LAYER_CHANNEL_COUNTS[$SLURM_ARRAY_TASK_ID]}
set num_batches_per_update=${BATCHES_PER_UPDATE_COUNTS[$SLURM_ARRAY_TASK_ID]}
set max_examples_per_cyclone=${MAX_EXAMPLES_PER_CYCLONE_COUNTS[$SLURM_ARRAY_TASK_ID]}

set model_dir_name="${TOP_MODEL_DIR_NAME}/num-first-layer-channels=${num_first_layer_channels}_num-batches-per-update=${num_batches_per_update}_max-examples-per-cyclone=${max_examples_per_cyclone}"
echo $model_dir_name

set CYCLONE_ID_STRINGS=("2017AL01" "2017AL02" "2017AL03" "2017AL04" "2017AL05" "2017AL06" "2017AL07" "2017AL08" "2017AL09" "2017AL10" "2017AL11" "2017AL12" "2017AL13" "2017AL14" "2017AL15" "2017AL16" "2017AL17" "2017AL18" "2017AL19" "2018AL01" "2018AL02" "2018AL03" "2018AL04" "2018AL05" "2018AL06" "2018AL07" "2018AL08" "2018AL09" "2018AL10" "2018AL11" "2018AL12" "2018AL13" "2019AL01" "2019AL02" "2019AL03" "2019AL04" "2019AL05" "2019AL06" "2019AL07" "2019AL08" "2019AL09" "2019AL10" "2019AL11" "2019AL12" "2019AL13" "2019AL14" "2019AL15" "2019AL16" "2019AL17" "2019AL18" "2019AL19" "2019AL20")
set i=1

while ($i <= ${#CYCLONE_ID_STRINGS})
    python3 -u "${CODE_DIR_NAME}/apply_neural_net.py" \
    --input_model_file_name="${model_dir_name}/model.h5" \
    --input_satellite_dir_name="${TRAINING_DIR_NAME}" \
    --cyclone_id_string="${CYCLONE_ID_STRINGS[$i]}" \
    --num_bnn_iterations=1 \
    --max_ensemble_size=100 \
    --data_aug_num_translations=8 \
    --output_dir_name="${model_dir_name}/training"

    @ i = $i + 1
end
