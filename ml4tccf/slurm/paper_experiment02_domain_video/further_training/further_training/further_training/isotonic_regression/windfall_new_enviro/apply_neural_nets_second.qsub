#!/bin/tcsh

#SBATCH --job-name="apply_neural_nets_second"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="gpuwf"
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
##SBATCH --nodes=1
##SBATCH --ntasks=1
##SBATCH --cpus-per-task=1
##SBATCH --ntasks-per-node=1
#SBATCH --time=08:00:00
#SBATCH --array=1-56
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=apply_neural_nets_second_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_standalone/ml4tccf"
set TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_models/paper_experiment02_domain_video/further_training/further_training/further_training"
set EXAMPLE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4tccf_project/satellite_data/processed/normalized_for_paper/800x800_grids"

set GRID_ROW_COUNTS=("300" "300" "300" "300" "300" "300" "300" "400" "400" "400" "400" "400" "400" "400" "500" "500" "500" "500" "500" "500" "500" "600" "600" "600" "600" "600" "600" "600" "300" "300" "300" "300" "300" "300" "300" "400" "400" "400" "400" "400" "400" "400" "500" "500" "500" "500" "500" "500" "500" "600" "600" "600" "600" "600" "600" "600")
set MAIN_POOLING_FACTORS=("2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3")
set LAG_TIME_COUNTS=("1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" "6" "7")

set num_grid_rows=${GRID_ROW_COUNTS[$SLURM_ARRAY_TASK_ID]}
set main_pooling_factor=${MAIN_POOLING_FACTORS[$SLURM_ARRAY_TASK_ID]}
set num_lag_times=${LAG_TIME_COUNTS[$SLURM_ARRAY_TASK_ID]}

set model_dir_name="${TOP_MODEL_DIR_NAME}/num-grid-rows=${num_grid_rows}_main-pooling-factor=${main_pooling_factor}_num-lag-times=${num_lag_times}"
echo $model_dir_name

set CYCLONE_ID_STRINGS=("2016WP13" "2016WP14" "2016WP15" "2016WP16" "2016WP17" "2016WP18" "2016WP19" "2016WP20" "2016WP21" "2016WP22" "2016WP23" "2016WP24" "2016WP25" "2016WP26" "2016WP27" "2016WP28" "2016WP29" "2016WP30" "2017WP01" "2017WP02" "2017WP03" "2017WP04" "2017WP05" "2017WP06" "2017WP07" "2017WP08" "2017WP09" "2017WP10" "2017WP11" "2017WP12" "2017WP13" "2017WP14" "2017WP15" "2017WP16" "2017WP17" "2017WP18" "2017WP19" "2017WP20" "2017WP21" "2017WP22" "2017WP23" "2017WP24" "2017WP25" "2017WP26" "2017WP27" "2017WP28" "2017WP29" "2017WP30" "2017WP31" "2017WP32" "2017WP33" "2018WP01" "2018WP02" "2018WP03" "2018WP04" "2018WP05" "2018WP06" "2018WP07" "2018WP08" "2018WP09" "2018WP10" "2018WP11" "2018WP12" "2018WP13" "2018WP14" "2018WP15" "2018WP16" "2018WP17" "2018WP18" "2018WP19" "2018WP20" "2018WP21" "2018WP22" "2018WP23" "2018WP24" "2018WP25" "2018WP26" "2018WP27" "2018WP28" "2018WP29" "2018WP30" "2018WP31" "2018WP32" "2018WP33" "2018WP34" "2018WP35" "2018WP36" "2019WP01" "2019WP02" "2019WP03" "2019WP04" "2019WP05" "2019WP06" "2019WP07" "2019WP08" "2019WP09" "2019WP10" "2019WP11" "2019WP12" "2019WP13" "2019WP14" "2019WP15" "2019WP16" "2019WP17" "2019WP18" "2019WP19" "2019WP20" "2019WP21" "2019WP22" "2019WP23" "2019WP24" "2019WP25" "2019WP26" "2019WP27" "2019WP28" "2019WP29" "2019WP30")
set i=1

while ($i <= ${#CYCLONE_ID_STRINGS})
    python3 -u "${CODE_DIR_NAME}/apply_neural_net.py" \
    --input_model_file_name="${model_dir_name}/model.h5" \
    --input_satellite_dir_name="${EXAMPLE_DIR_NAME}" \
    --cyclone_id_string="${CYCLONE_ID_STRINGS[$i]}" \
    --num_bnn_iterations=1 \
    --max_ensemble_size=100 \
    --data_aug_num_translations=8 \
    --output_dir_name="${model_dir_name}/training"
    
    @ i = $i + 1
end
